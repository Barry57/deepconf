"""
HumanEval workflow（含置信度）
1. 下载官方题库
2. vLLM 生成 200 条路径
3. dump token 级置信度
4. 生成官方评测格式
5. 打印 Pass@k
"""
import os, sys, json, time, argparse, numpy as np
import urllib.request
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer

HUMAN_EVAL_URL = "https://github.com/openai/human-eval/raw/master/data/HumanEval.jsonl.gz"
HUMAN_EVAL_FILE = "HumanEval.jsonl"


# ---------- 1. 自动下载题库 ----------
def maybe_download():
    if os.path.exists(HUMAN_EVAL_FILE):
        return
    print("↓ 下载 HumanEval 题库...")
    urllib.request.urlretrieve(HUMAN_EVAL_URL, "HumanEval.jsonl.gz")
    os.system("gunzip -c HumanEval.jsonl.gz > HumanEval.jsonl")
    os.remove("HumanEval.jsonl.gz")


# ---------- 2. 生成 ----------
def generate_main(model_path, n_samples=200, temperature=0.6, max_tokens=512):
    maybe_download()

    print("加载 tokenizer & vLLM...")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    llm = LLM(model=model_path,
              tensor_parallel_size=1,
              enable_prefix_caching=True,
              trust_remote_code=True)

    sampling_params = SamplingParams(
        n=n_samples,
        temperature=temperature,
        top_p=0.95,
        max_tokens=max_tokens,
        logprobs=20,               # 保留置信度
    )

    samples = []
    for line in open(HUMAN_EVAL_FILE):
        prob = json.loads(line)
        prompt = prob["prompt"]          # 签名+docstring
        outputs = llm.generate([prompt], sampling_params)

        traces = []
        for out in outputs[0].outputs:
            tokens = [o.token for o in out.logprobs]
            scores = [o.logprob for o in out.logprobs]
            traces.append({
                "text": out.text,
                "token_scores": list(zip(tokens, scores))
            })

        # 选平均 logprob 最高的一条作为最终答案
        best_text = max(traces, key=lambda x: np.mean([s for _, s in x["token_scores"]]))["text"]
        samples.append({
            "task_id": prob["task_id"],
            "completion": best_text,      # 官方评测只认这个
            "all_traces": traces          # 置信度留底
        })

        if len(samples) % 20 == 0:
            print(f"✅ 完成 {len(samples)}/164")

    return samples


# ---------- 3. 官方评测 ----------
def evaluate(samples, k_list=[1, 10, 100]):
    # 安装评测器（仅首次）
    os.system("pip -q install human-eval")
    from human_eval.execution import check_correctness
    from human_eval.data import HUMAN_EVAL, read_problems

    problems = read_problems()
    result = {f"pass@{k}": [] for k in k_list}

    for s in samples:
        prob = problems[s["task_id"]]
        completion = s["completion"]
        # 拼装 & 执行
        res = check_correctness(prob, completion, timeout=3.0)
        result["pass@1"].append(1 if res["passed"] else 0)

    # 计算 Pass@k（简化版，假设 n_samples>=max(k)）
    def estimator(pass_flags, n, k):
        """pass@k 无偏估计"""
        import math
        s = sum(pass_flags)
        return 1.0 - math.comb(n - s, k) / math.comb(n, k)

    n = len(samples[0]["all_traces"])
    return {f"pass@{k}": estimator(result["pass@1"], n, k) for k in k_list}


# ---------- 4. main ----------
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str,
                        default="Qwen/Qwen3-Coder-30B-A3B-Instruct",
                        help="Hub 模型 ID 或本地目录")
    parser.add_argument("--out", default="humaneval_qwen30b_conf.jsonl")
    parser.add_argument("--n_samples", type=int, default=200)
    parser.add_argument("--temperature", type=float, default=0.6)
    parser.add_argument("--tp", type=int, default=4,
                        help="tensor_parallel_size，30B 建议 4 卡")
    args = parser.parse_args()
    global TP_SIZE
    TP_SIZE = args.tp

    print("🚀 开始生成...")
    samples = generate_main(args.model, args.n_samples, args.temperature)

    with open(args.out, "w", encoding="utf-8") as f:
        for s in samples:
            f.write(json.dumps(s, ensure_ascii=False) + "\n")
    print(f"💾 已保存含置信度样本 -> {args.out}")

    # 可选：直接评测
    print("⏳ 执行官方评测（3 秒/题）...")
    pass_k = evaluate(samples)
    print("📊 结果：", pass_k)


if __name__ == "__main__":
    main()
