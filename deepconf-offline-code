"""
HumanEval workflowï¼ˆå«ç½®ä¿¡åº¦ï¼‰
1. ä¸‹è½½å®˜æ–¹é¢˜åº“
2. vLLM ç”Ÿæˆ 200 æ¡è·¯å¾„
3. dump token çº§ç½®ä¿¡åº¦
4. ç”Ÿæˆå®˜æ–¹è¯„æµ‹æ ¼å¼
5. æ‰“å° Pass@k
"""
import os, sys, json, time, argparse, numpy as np
import urllib.request
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer

HUMAN_EVAL_URL = "https://github.com/openai/human-eval/raw/master/data/HumanEval.jsonl.gz"
HUMAN_EVAL_FILE = "HumanEval.jsonl"


# ---------- 1. è‡ªåŠ¨ä¸‹è½½é¢˜åº“ ----------
def maybe_download():
    if os.path.exists(HUMAN_EVAL_FILE):
        return
    print("â†“ ä¸‹è½½ HumanEval é¢˜åº“...")
    urllib.request.urlretrieve(HUMAN_EVAL_URL, "HumanEval.jsonl.gz")
    os.system("gunzip -c HumanEval.jsonl.gz > HumanEval.jsonl")
    os.remove("HumanEval.jsonl.gz")


# ---------- 2. ç”Ÿæˆ ----------
def generate_main(model_path, n_samples=200, temperature=0.6, max_tokens=512):
    maybe_download()

    print("åŠ è½½ tokenizer & vLLM...")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    llm = LLM(model=model_path,
              tensor_parallel_size=1,
              enable_prefix_caching=True,
              trust_remote_code=True)

    sampling_params = SamplingParams(
        n=n_samples,
        temperature=temperature,
        top_p=0.95,
        max_tokens=max_tokens,
        logprobs=20,               # ä¿ç•™ç½®ä¿¡åº¦
    )

    samples = []
    for line in open(HUMAN_EVAL_FILE):
        prob = json.loads(line)
        prompt = prob["prompt"]          # ç­¾å+docstring
        outputs = llm.generate([prompt], sampling_params)

        traces = []
        for out in outputs[0].outputs:
            tokens = [o.token for o in out.logprobs]
            scores = [o.logprob for o in out.logprobs]
            traces.append({
                "text": out.text,
                "token_scores": list(zip(tokens, scores))
            })

        # é€‰å¹³å‡ logprob æœ€é«˜çš„ä¸€æ¡ä½œä¸ºæœ€ç»ˆç­”æ¡ˆ
        best_text = max(traces, key=lambda x: np.mean([s for _, s in x["token_scores"]]))["text"]
        samples.append({
            "task_id": prob["task_id"],
            "completion": best_text,      # å®˜æ–¹è¯„æµ‹åªè®¤è¿™ä¸ª
            "all_traces": traces          # ç½®ä¿¡åº¦ç•™åº•
        })

        if len(samples) % 20 == 0:
            print(f"âœ… å®Œæˆ {len(samples)}/164")

    return samples


# ---------- 3. å®˜æ–¹è¯„æµ‹ ----------
def evaluate(samples, k_list=[1, 10, 100]):
    # å®‰è£…è¯„æµ‹å™¨ï¼ˆä»…é¦–æ¬¡ï¼‰
    os.system("pip -q install human-eval")
    from human_eval.execution import check_correctness
    from human_eval.data import HUMAN_EVAL, read_problems

    problems = read_problems()
    result = {f"pass@{k}": [] for k in k_list}

    for s in samples:
        prob = problems[s["task_id"]]
        completion = s["completion"]
        # æ‹¼è£… & æ‰§è¡Œ
        res = check_correctness(prob, completion, timeout=3.0)
        result["pass@1"].append(1 if res["passed"] else 0)

    # è®¡ç®— Pass@kï¼ˆç®€åŒ–ç‰ˆï¼Œå‡è®¾ n_samples>=max(k)ï¼‰
    def estimator(pass_flags, n, k):
        """pass@k æ— åä¼°è®¡"""
        import math
        s = sum(pass_flags)
        return 1.0 - math.comb(n - s, k) / math.comb(n, k)

    n = len(samples[0]["all_traces"])
    return {f"pass@{k}": estimator(result["pass@1"], n, k) for k in k_list}


# ---------- 4. main ----------
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str,
                        default="Qwen/Qwen3-Coder-30B-A3B-Instruct",
                        help="Hub æ¨¡å‹ ID æˆ–æœ¬åœ°ç›®å½•")
    parser.add_argument("--out", default="humaneval_qwen30b_conf.jsonl")
    parser.add_argument("--n_samples", type=int, default=200)
    parser.add_argument("--temperature", type=float, default=0.6)
    parser.add_argument("--tp", type=int, default=4,
                        help="tensor_parallel_sizeï¼Œ30B å»ºè®® 4 å¡")
    args = parser.parse_args()
    global TP_SIZE
    TP_SIZE = args.tp

    print("ğŸš€ å¼€å§‹ç”Ÿæˆ...")
    samples = generate_main(args.model, args.n_samples, args.temperature)

    with open(args.out, "w", encoding="utf-8") as f:
        for s in samples:
            f.write(json.dumps(s, ensure_ascii=False) + "\n")
    print(f"ğŸ’¾ å·²ä¿å­˜å«ç½®ä¿¡åº¦æ ·æœ¬ -> {args.out}")

    # å¯é€‰ï¼šç›´æ¥è¯„æµ‹
    print("â³ æ‰§è¡Œå®˜æ–¹è¯„æµ‹ï¼ˆ3 ç§’/é¢˜ï¼‰...")
    pass_k = evaluate(samples)
    print("ğŸ“Š ç»“æœï¼š", pass_k)


if __name__ == "__main__":
    main()
